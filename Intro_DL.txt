@딥러닝 개론

인공지능 ) 머신러닝 ) 딥러닝

#Machine Learning
-트리계열
Decision tree - 의사결정

-SVM
support vector machine - 빠른
직선에서 가장 수직거리가 가까운 점이 서포트벡터
직선과 서포트벡터 사이의 거리인 마진을 최대화

-신경망
히든 레이어(은닉층)의 수가 많은 구조의 신경망이 딥러닝

#Deep Learning
깊은 신경망 구조의 머신러닝
input - (((신경망))) - output
end to end(종단간 구조)
종류: MLP(multi-layer-perceptron), CNN, RNN, Transformer

Transformer: self-attention 구조 사용

딥러닝: 성능 좋음(예측력 높음)
머신러닝: 해석력 좋음, 어떤 특성이 어떤 역할을 하는지, 어떤 특성이 중요한지 알고 이를 조절해 예측력을 높일 수 있음

#학습
Command: if/else가 핵심
Learning/training: 모델이 데이터를 받아 예측, 정답과의 오차를 계산해 피드백(학습), 예측력이 높은 모델로 계속 변화 - 개발자가 예측 못한 인풋도 처리가능
깊은 신경망: 은닉층의 개수가 많아지면 복잡한 특성을 추출하고 학습 할 수 있음
학습: 점점 더 좋은 예측을 하는 모델로 만들어가는 것

-데이터셋 분할
객관적인 평가를 위해 train set과 test set으로 split
train set for 학습, test set for 평가 (8:2)
분할방법: random(shuffle 중요), stratify(층화 추출; 클래스 별로 추출)
validation set: train set에서 한번 더 분할(0.2), 학습 중 일반화 성능 평가를 위함, validation 성능이 감소하는 구간이 생김(overfitting, 과적합 구간),
                hyperparameter를 어떤 식으로 바꿔가며 실험을 해야 학습이 잘 될지 지표 만드는 역할

-a project using CIFAR10 dataset
동물 사진 분류

-lable(정답)
label --학습--> model
ground-truth: 정확한 레이블, 실제값

-지도학습(supervised learning)
dataset = data + label
대부분의 경우 지도학습

-비지도학습(unsupervised learning)
dataset = data
labeling -> 비용 많이 듦
labeling이 까다로운 경우 비지도학습이 유용
ex) k-means clusting(군집화)

-반지도학습(semi-supervised learning)
ex) pseudo-labeling: label을 model에 지도학습, label이 없는 데이터셋을 model을 이용해서 labeling(예측값) with confidence 참고(기준을 넘긴 값만 사용),
                     이렇게 만든 데이터셋을 기존의 데이터셋에 추가하여 다시 모델 학습
                     (많은 데이터셋 -> 성능향상)

-선형회귀(linear regression)
Linear Regression: 선을 그어 값을 예측한다
a. 데이터를 일반화하는 선을 찾는다
b. 그 선을 이용해 새로운 데이터를 예측한다
y=ax+b, x=10이라면 예측값=10a+b

신경망 입장에서, (x 변수임, 곱하기 아님!!)
H(x) = w1x1 +w2x2+w3x3 +bias
(w = 가중치, x=특성)

선형회귀 구조는 예측이 그 자체로서 완료되는 형태

-편향(bias)
b, y절편
bias를 반영해야 예측력이 더 좋아짐

-회귀직선
H(x)=Wx+b
(H=hypothesis=예측값, W=가중치, b=bias, x=특성값)
error = H(x)-y
(y=label)

-비용함수(cost function)
cost=1/n<시그마>(H(x^i)-y^i)^2
=1/n<시그마>에러^2
=에러들의 합

-입력과 출력
특성들(input) --가중합+b--> 새로운 특성(output)

-신경망(neural networks)

-선형변환
입력(10) -> f:2x+1 -> g:-x+10 -> 출력(-11)
>> h(x) = -(2x+1)+10 = -2x+9 (f와 g의 합성함수)
입력(10) -> h -> 출력(-11)
수많은 레이어를 쌓아도 하나의 레이어로 치환가능
결국은 하나의 레이어를 가진 신경망이 되어 깊은 신경망으로서의 의미x
깊이 유지x --해결--> 비선형변형 추가

-비선형변환
sigmoid
sigmoid(x)=1/1+e^-x (e: 2.718)
input -> f -> 비선형변환 -> g -> output
비선형변환을 거치면 0~1 사이의 값이 나옴
(보통 0 혹은 1에 가까운 값이 나옴)
스위치 역할

선형변환: 스케일의 변화, 기존 데이터들의 관계가 유지
비선형변환: 기존 데이터들의 관계가 유지되지 않음

-Activation
이전 층 계산 값을 비선형변환한 값

-활성화 함수
ex) Sigmoid, ReLU
ReLU: x < 0, f(x) = 0
      x >= 0, f(x) = x
장점: 계산이 단순, gradient vanishing 문제x

Sigmoid는 기울기 소실(gradient vanishing) 문제점 존재
가중치를 업데이트 할 때 기울기 정보를 사용하는데 역전파가 될 때 가중치에 대한 기울기 뿐만 아니라 활성화 함수에 대한 기울기도 함께 계산됨

-데이터 행렬
one-hot encoding
ex) 서울: 0, 광역시: 1, 지방: 2
-> 단순한 인덱싱인데 수학적으로는 지방 = 광역시 x2 로 해석될 여지가 있음
따라서 (1, 0, 0) / (0, 1, 0) / (0, 0, 1) 식으로 바꿔줌

-데이터 행렬과 가중치 행렬(weight matrix)
data [1 2 3]
     [4 5 6]
weight [1]
       [1]
       [0]
 행렬곱 = 1x1 + 2x1 + 3x0 = 3
                4x1 + 5x1 + 6x0 = 9
          [3]
          [9]

-데이터 행렬에 행 추가
데이터 포인터 추가, 가중치 그대로
-데이터 행렬에 열 추가
데이터 포인터 그대로, 가중치 추가

-경사하강법(gradient descent)
단계적으로 오차함수를 조금씩 줄여가며 반복적으로 가중치를 개선해 가며 최적의 가중치를 찾아가는 방법

-비용함수
convex

-비용함수 업데이트
새로운 w = 원래 w + (기울기 반대방향)
기울기가 음수이면 오른쪽(+), 기울기가 양수이면 왼쪽(-)
w := w - @(ac)/(aw)
ac/aw: 기울기, cost/w
(w: 가중치, a: 편미분 기호, @: 학습률 상수)

-학습률(learning rate, lr)
학습률이 너무 높을 경우: 많은 이동으로 최적값을 지나 업데이트 될 가능성 높음, 오히려 최적값에서 멀어짐
학습률이 너무 낮은 경우: 조금씩 이동해서 시간이 오래걸림

-미분과 편미분(partial differential)
ex) y = 2x1 + 3x2
미분 dy/dx -> x1, x2에 대해 모두 미분
편미분 ay/ax1 -> 2 (x1에 대해서만 미분, x2는 상수취급)

-Optimizer: 최적화 방법

-Gradient Descent
기울기를 구해서 기울기의 반대방향으로 학습률과 기울기의 곱만큼 이동시킨다

-SGD(Stochastic Gradient Descent)
Batch GD
수많은 데이터를 신경망에 통과시킨 후 오차들을 더해 cost를 구한 뒤 업데이트 진행, Batch란 데이터세트 전체의 개수, 데이터세트 한바퀴 돌고 난 후 가중치 업데이트 진행

Mini-batch GD / Single sample GD
미니: Batch를 여러 개로 쪼갠 후 각각 loss를 구한 후 업데이트 진행
(cost: batch에 대한 에러의 총합, loss: mini-batch에 대한 에러의 총합)
싱글: 하나의 샘플에 대해 에러를 계산하고 그것을 바로 업데이트

(Stochastic: 확률적, 무작위)

-Momentum
SGD + Momentun(관성)
local minimum을 global minimum으로 착각하기 쉬움 --> 관성의 도움을 받음
torch.optim.SGD(..., momentum=0.9)

-AdaGrad
SGD + adaptive 학습률 감소(learning rate decay)

-RMSprop
AdaGRad + gradient decay

-Adam
가장 유명, 자주 쓰임, 디폴트
Adagrad(RMSprop) + Momentum
