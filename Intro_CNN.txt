-grayscale
흑백 pixel
value: 0~255 (black<->white)
          : 0~1 <- normalization

-RGB
channel: red, green, blue
육면체가 있을 때, 높이 h, 가로 a, 세로 c
c가 channel
유사) HSV, Lab

-이미지 필터(filters)
sharpen filter, blur filter...
filter = kernel

 필터 연산
filter * image
*: pixel-wise, 같은 위치끼리 곱하고 그 곱을 다 더함
output image의 첫번째 픽셀에 그 값 적용
실습링크: https://setosa.io/ev/image-kernels/

-합성곱 연산(convolution)
신경망에 이미지 입력
ex) 5 by 5 pixel -> 25개의 특성(input)으로
신경망을 거쳐 하나의 값 출력
지역적 위치 정보 사라짐 -> 현재 거의 사용 x -> 합성곱 연산 사용

Convolution
5 by 5 --convolution--> 3 by 3 result
result는 압축된 값을 가지고 있음 (+regional info)
단순히 픽셀들을 입력으로 넣는 것과는 다름
레이어를 거치면서 input 이미지 상에서의 region의 크기 점차 증 -> 최종적으로 상세하고 많은 정보를 가진 feature를 통해 예측 진행

Pixel matrix: 이미지를 pixel의 value들이 존재하는 matrix 형태로 표현

padding
zero padding: 0으로 한 바퀴를 감싸주는 기법
거의 필수적으로 사용
input size를 동일하게 유지할 수 있음

stride
연산할 때 하나의 pixel이 아닌 n개의 pixel을 이동시킴

-Chanel
(c, w, h)
(3, 256. 256) -- filter: 5개 --> (5, 256, 256) -- filter: 20개 --> (20, 256, 256)
feature map 생성

-Max poling
4x4 -- max pooling --> 2x2
ex)
150 125 100 80
110 90 140 58
85 120 75 120
230 210 140 190

150 140
230 190

자매품) averge pooling(평균값 사용)

parameter 수를 줄이기 위해 사용

-채널 계산
224x224x64 --> 224x224x128
max pooling + 128개 필터 사용됨

-CNN
신경망과 CNN
n개의 filter, n개의 feature map, n개의 채널

fc layer node 개수 = 예측을 진행할 class 수

-CNN 코드
// 신경망 구조

// 모델정의
class Net(nn.module): // 상속받음
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, 5) // input channel, output channel, kernel size
        self.conv2 = nn.Conv2d(64, 128, 5)
        self.fc = nn.Linear(128*5*5, 10)

// 구성해놓은 레이어들을 어떻게 연결시킬 것인지
def forward(self, x):
    x = F.relu(self.con1(x)) // 비선형 변환
    x = F.max_pool2d(x, 2) // 2 by 2 pooling
    x = F.relu(self.conv2(x))
    x = F.max_pool2d(x, 2)
    x = torch.flatten(x, 1) // 한 줄로 펴서
    x = self.fc(x) // fc 레이어 통과시킴
    return x

// 실행
net = Net() // object 생성
output = net(input) // 출력 계산
loss = criterion(output, target) // 출력과 정답(target)을 이용해 loss 계산
loss.backward() // 역전파
optimizer.step() // 가중치 업데이트
